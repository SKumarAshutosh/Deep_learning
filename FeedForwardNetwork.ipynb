{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzSCYXixXHhZ7Xbv5u5VRZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SKumarAshutosh/Deep_learning/blob/main/FeedForwardNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Load the dataset:\n",
        "\n",
        "**Variables Introduced:**\n",
        "\n",
        "\n",
        "* iris: The dataset containing features and labels of the iris dataset.\n",
        "* X: The feature matrix containing measurements of iris flowers.\n",
        "* y: The target labels indicating the species of the iris flowers."
      ],
      "metadata": {
        "id": "CzGBDeF2G7xq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Load the dataset**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Blchwy-HJkQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports required modules.\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "metadata": {
        "id": "AITf_96sIoWy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loads the iris dataset\n",
        "#iris: Contains the dataset with features and labels of iris flowers.\n",
        "iris = datasets.load_iris()"
      ],
      "metadata": {
        "id": "6ihnDA5BIwhX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCiIFIBCR7xo",
        "outputId": "e61d2e59-ecc2-4e9d-be35-0213ed709142"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
              "        [4.9, 3. , 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.3, 0.2],\n",
              "        [4.6, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.6, 1.4, 0.2],\n",
              "        [5.4, 3.9, 1.7, 0.4],\n",
              "        [4.6, 3.4, 1.4, 0.3],\n",
              "        [5. , 3.4, 1.5, 0.2],\n",
              "        [4.4, 2.9, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.1],\n",
              "        [5.4, 3.7, 1.5, 0.2],\n",
              "        [4.8, 3.4, 1.6, 0.2],\n",
              "        [4.8, 3. , 1.4, 0.1],\n",
              "        [4.3, 3. , 1.1, 0.1],\n",
              "        [5.8, 4. , 1.2, 0.2],\n",
              "        [5.7, 4.4, 1.5, 0.4],\n",
              "        [5.4, 3.9, 1.3, 0.4],\n",
              "        [5.1, 3.5, 1.4, 0.3],\n",
              "        [5.7, 3.8, 1.7, 0.3],\n",
              "        [5.1, 3.8, 1.5, 0.3],\n",
              "        [5.4, 3.4, 1.7, 0.2],\n",
              "        [5.1, 3.7, 1.5, 0.4],\n",
              "        [4.6, 3.6, 1. , 0.2],\n",
              "        [5.1, 3.3, 1.7, 0.5],\n",
              "        [4.8, 3.4, 1.9, 0.2],\n",
              "        [5. , 3. , 1.6, 0.2],\n",
              "        [5. , 3.4, 1.6, 0.4],\n",
              "        [5.2, 3.5, 1.5, 0.2],\n",
              "        [5.2, 3.4, 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.6, 0.2],\n",
              "        [4.8, 3.1, 1.6, 0.2],\n",
              "        [5.4, 3.4, 1.5, 0.4],\n",
              "        [5.2, 4.1, 1.5, 0.1],\n",
              "        [5.5, 4.2, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.2, 1.2, 0.2],\n",
              "        [5.5, 3.5, 1.3, 0.2],\n",
              "        [4.9, 3.6, 1.4, 0.1],\n",
              "        [4.4, 3. , 1.3, 0.2],\n",
              "        [5.1, 3.4, 1.5, 0.2],\n",
              "        [5. , 3.5, 1.3, 0.3],\n",
              "        [4.5, 2.3, 1.3, 0.3],\n",
              "        [4.4, 3.2, 1.3, 0.2],\n",
              "        [5. , 3.5, 1.6, 0.6],\n",
              "        [5.1, 3.8, 1.9, 0.4],\n",
              "        [4.8, 3. , 1.4, 0.3],\n",
              "        [5.1, 3.8, 1.6, 0.2],\n",
              "        [4.6, 3.2, 1.4, 0.2],\n",
              "        [5.3, 3.7, 1.5, 0.2],\n",
              "        [5. , 3.3, 1.4, 0.2],\n",
              "        [7. , 3.2, 4.7, 1.4],\n",
              "        [6.4, 3.2, 4.5, 1.5],\n",
              "        [6.9, 3.1, 4.9, 1.5],\n",
              "        [5.5, 2.3, 4. , 1.3],\n",
              "        [6.5, 2.8, 4.6, 1.5],\n",
              "        [5.7, 2.8, 4.5, 1.3],\n",
              "        [6.3, 3.3, 4.7, 1.6],\n",
              "        [4.9, 2.4, 3.3, 1. ],\n",
              "        [6.6, 2.9, 4.6, 1.3],\n",
              "        [5.2, 2.7, 3.9, 1.4],\n",
              "        [5. , 2. , 3.5, 1. ],\n",
              "        [5.9, 3. , 4.2, 1.5],\n",
              "        [6. , 2.2, 4. , 1. ],\n",
              "        [6.1, 2.9, 4.7, 1.4],\n",
              "        [5.6, 2.9, 3.6, 1.3],\n",
              "        [6.7, 3.1, 4.4, 1.4],\n",
              "        [5.6, 3. , 4.5, 1.5],\n",
              "        [5.8, 2.7, 4.1, 1. ],\n",
              "        [6.2, 2.2, 4.5, 1.5],\n",
              "        [5.6, 2.5, 3.9, 1.1],\n",
              "        [5.9, 3.2, 4.8, 1.8],\n",
              "        [6.1, 2.8, 4. , 1.3],\n",
              "        [6.3, 2.5, 4.9, 1.5],\n",
              "        [6.1, 2.8, 4.7, 1.2],\n",
              "        [6.4, 2.9, 4.3, 1.3],\n",
              "        [6.6, 3. , 4.4, 1.4],\n",
              "        [6.8, 2.8, 4.8, 1.4],\n",
              "        [6.7, 3. , 5. , 1.7],\n",
              "        [6. , 2.9, 4.5, 1.5],\n",
              "        [5.7, 2.6, 3.5, 1. ],\n",
              "        [5.5, 2.4, 3.8, 1.1],\n",
              "        [5.5, 2.4, 3.7, 1. ],\n",
              "        [5.8, 2.7, 3.9, 1.2],\n",
              "        [6. , 2.7, 5.1, 1.6],\n",
              "        [5.4, 3. , 4.5, 1.5],\n",
              "        [6. , 3.4, 4.5, 1.6],\n",
              "        [6.7, 3.1, 4.7, 1.5],\n",
              "        [6.3, 2.3, 4.4, 1.3],\n",
              "        [5.6, 3. , 4.1, 1.3],\n",
              "        [5.5, 2.5, 4. , 1.3],\n",
              "        [5.5, 2.6, 4.4, 1.2],\n",
              "        [6.1, 3. , 4.6, 1.4],\n",
              "        [5.8, 2.6, 4. , 1.2],\n",
              "        [5. , 2.3, 3.3, 1. ],\n",
              "        [5.6, 2.7, 4.2, 1.3],\n",
              "        [5.7, 3. , 4.2, 1.2],\n",
              "        [5.7, 2.9, 4.2, 1.3],\n",
              "        [6.2, 2.9, 4.3, 1.3],\n",
              "        [5.1, 2.5, 3. , 1.1],\n",
              "        [5.7, 2.8, 4.1, 1.3],\n",
              "        [6.3, 3.3, 6. , 2.5],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [7.1, 3. , 5.9, 2.1],\n",
              "        [6.3, 2.9, 5.6, 1.8],\n",
              "        [6.5, 3. , 5.8, 2.2],\n",
              "        [7.6, 3. , 6.6, 2.1],\n",
              "        [4.9, 2.5, 4.5, 1.7],\n",
              "        [7.3, 2.9, 6.3, 1.8],\n",
              "        [6.7, 2.5, 5.8, 1.8],\n",
              "        [7.2, 3.6, 6.1, 2.5],\n",
              "        [6.5, 3.2, 5.1, 2. ],\n",
              "        [6.4, 2.7, 5.3, 1.9],\n",
              "        [6.8, 3. , 5.5, 2.1],\n",
              "        [5.7, 2.5, 5. , 2. ],\n",
              "        [5.8, 2.8, 5.1, 2.4],\n",
              "        [6.4, 3.2, 5.3, 2.3],\n",
              "        [6.5, 3. , 5.5, 1.8],\n",
              "        [7.7, 3.8, 6.7, 2.2],\n",
              "        [7.7, 2.6, 6.9, 2.3],\n",
              "        [6. , 2.2, 5. , 1.5],\n",
              "        [6.9, 3.2, 5.7, 2.3],\n",
              "        [5.6, 2.8, 4.9, 2. ],\n",
              "        [7.7, 2.8, 6.7, 2. ],\n",
              "        [6.3, 2.7, 4.9, 1.8],\n",
              "        [6.7, 3.3, 5.7, 2.1],\n",
              "        [7.2, 3.2, 6. , 1.8],\n",
              "        [6.2, 2.8, 4.8, 1.8],\n",
              "        [6.1, 3. , 4.9, 1.8],\n",
              "        [6.4, 2.8, 5.6, 2.1],\n",
              "        [7.2, 3. , 5.8, 1.6],\n",
              "        [7.4, 2.8, 6.1, 1.9],\n",
              "        [7.9, 3.8, 6.4, 2. ],\n",
              "        [6.4, 2.8, 5.6, 2.2],\n",
              "        [6.3, 2.8, 5.1, 1.5],\n",
              "        [6.1, 2.6, 5.6, 1.4],\n",
              "        [7.7, 3. , 6.1, 2.3],\n",
              "        [6.3, 3.4, 5.6, 2.4],\n",
              "        [6.4, 3.1, 5.5, 1.8],\n",
              "        [6. , 3. , 4.8, 1.8],\n",
              "        [6.9, 3.1, 5.4, 2.1],\n",
              "        [6.7, 3.1, 5.6, 2.4],\n",
              "        [6.9, 3.1, 5.1, 2.3],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [6.8, 3.2, 5.9, 2.3],\n",
              "        [6.7, 3.3, 5.7, 2.5],\n",
              "        [6.7, 3. , 5.2, 2.3],\n",
              "        [6.3, 2.5, 5. , 1.9],\n",
              "        [6.5, 3. , 5.2, 2. ],\n",
              "        [6.2, 3.4, 5.4, 2.3],\n",
              "        [5.9, 3. , 5.1, 1.8]]),\n",
              " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " 'frame': None,\n",
              " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
              " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
              " 'feature_names': ['sepal length (cm)',\n",
              "  'sepal width (cm)',\n",
              "  'petal length (cm)',\n",
              "  'petal width (cm)'],\n",
              " 'filename': 'iris.csv',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracts the feature data from the iris dataset.\n",
        "#X: The feature matrix containing measurements of iris flowers.\n",
        "X = iris.data\n"
      ],
      "metadata": {
        "id": "vfOsYuhrHwBU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracts target labels and reshapes them into a column vector.\n",
        "#y: The target labels indicating the iris flower species.\n",
        "y = iris.target.reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "lgQRFVe-JItU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. Why reshape with (-1, 1)?**\n",
        "\n",
        "reshape(-1, 1) means to reshape the data such that it has many rows as needed to maintain the number of elements and exactly one column. This is used to convert the flat array of target labels into a column vector."
      ],
      "metadata": {
        "id": "si8Wc89mP4SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#One-hot encodes the target labels.\n",
        "# encoder: Instance of OneHotEncoder.\n",
        "# y_onehot: One-hot encoded target labels.\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_onehot = encoder.fit_transform(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4uWp3DpJSXN",
        "outputId": "605ead58-e5f7-44f4-c3e0-a7fadcb743f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Why the need for one-hot encoding?**\n",
        "\n",
        "The iris dataset contains three classes: Setosa, Versicolour, and Virginica. These are represented as 0, 1, and 2 in the target array. For a multi-class classification using neural networks, it's often recommended to use one-hot encoding to represent class labels. One-hot encoding transforms categorical data into a format that can be more easily understood by the model, by representing each class with a binary vector."
      ],
      "metadata": {
        "id": "1fZ2zEajPa_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. What is sparse and why is it False?**\n",
        "\n",
        "In OneHotEncoder, the sparse argument determines if the returned array should be a sparse matrix or a dense numpy array. sparse=False means we get a dense array, which is easier to work with in this context."
      ],
      "metadata": {
        "id": "Yx2Zaer6QLjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Split the dataset into training and testing sets**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D6FIYwk2JuAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports the module to split datasets.\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "NLV_unYiJdiG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splits the dataset into training and testing subsets.\n",
        "# X_train, X_test: Training and testing feature matrices.\n",
        "# y_train, y_test: Training and testing target matrices\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "w4ArTx_2J8jw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q Why random_state=42?**\n",
        "\n",
        "The random_state is a seed for the random number generator. By setting it to a fixed value (like 42), the train/test split will always be deterministic. This ensures that the results are reproducible across different runs."
      ],
      "metadata": {
        "id": "d6_ck-G8QnAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Build the feed-forward neural network:**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c3xtYfNSKOCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports required TensorFlow and Keras modules.\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n"
      ],
      "metadata": {
        "id": "WA8hMK8HKGr3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializes a linear stack of layers for the neural network.\n",
        "#model: The neural network model.\n",
        "model = Sequential()\n"
      ],
      "metadata": {
        "id": "lbS4VGSiKTw9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequential():** This is used to initialize a linear stack of network layers. It allows you to build a model layer by layer.\n",
        "\n",
        "Dense: This is the layer type. Dense is a standard layer type that works for most cases. In a dense layer, all nodes in the previous layer connect to the nodes in the current layer.\n",
        "\n",
        "The first parameter, 10, is the number of neurons/nodes the layer has. For the input layer, you must also define the input_dim parameter, specifying the number of inputs (in this case, 4 inputs for the Iris dataset).\n",
        "\n",
        "activation: This is the activation function for the layer. The activation function decides whether a neuron should be activated based on the weighted sum. Here, we're using relu (Rectified Linear Activation) for our hidden layer and softmax for our output layer because it's a multi-class classification problem."
      ],
      "metadata": {
        "id": "-DtZ3ToJSmKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adds an input and hidden layer with 10 nodes and a ReLU activation function.\n",
        "model.add(Dense(10, input_dim=4, activation='relu'))\n"
      ],
      "metadata": {
        "id": "5HO1eb_UKnmR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is the input dimension 4?**\n",
        "\n",
        "The iris dataset has 4 features (sepal length, sepal width, petal length, petal width) for each data point. Hence, the input dimension is set to 4."
      ],
      "metadata": {
        "id": "ILFtwRxNQwqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adds an output layer with 3 nodes (one for each iris species) and a softmax activation function.\n",
        "model.add(Dense(3, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "Wqf7NXvqKucE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q Why use ReLU for the first activation and softmax for the second?**\n",
        "\n",
        "ReLU (Rectified Linear Unit): It's a commonly used activation function in hidden layers because it introduces non-linearity without being computationally expensive.\n",
        "\n",
        "Softmax: It's used in the output layer of multi-class classification tasks. It converts the raw output values (logits) from the network into probability distributions over the classes."
      ],
      "metadata": {
        "id": "lb3JgSRkREbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Compile the model:**"
      ],
      "metadata": {
        "id": "X4Rev58TK_Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Configures the model for training by setting the optimizer, loss function, and evaluation metric.\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "9duYihNTK6MS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why choose the Adam optimizer and categorical_crossentropy loss?\n",
        "\n",
        "Adam optimizer: Combines the best properties of the AdaGrad and RMSprop algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems. It's computationally efficient and has little memory requirement.\n",
        "categorical_crossentropy loss: It's the recommended loss for multi-class classification problems. It measures the difference between the true labels and the predicted probabilities."
      ],
      "metadata": {
        "id": "mvorwYMMRWCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Train the model:**"
      ],
      "metadata": {
        "id": "-JSAj2HNLQ31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Trains the model for 50 epochs using the training data and validates using the testing data.\n",
        "#history: Contains the training history, like loss and accuracy values at each epoch.\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=10, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh7MZfyFLM4f",
        "outputId": "6550b7c5-fe84-405c-aef1-6061e95e2518"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "12/12 [==============================] - 1s 24ms/step - loss: 1.8915 - accuracy: 0.3333 - val_loss: 1.7916 - val_accuracy: 0.3333\n",
            "Epoch 2/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6454 - accuracy: 0.3333 - val_loss: 1.5711 - val_accuracy: 0.3333\n",
            "Epoch 3/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4494 - accuracy: 0.3333 - val_loss: 1.3957 - val_accuracy: 0.3333\n",
            "Epoch 4/50\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3023 - accuracy: 0.3333 - val_loss: 1.2618 - val_accuracy: 0.3333\n",
            "Epoch 5/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1840 - accuracy: 0.3333 - val_loss: 1.1645 - val_accuracy: 0.3333\n",
            "Epoch 6/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1004 - accuracy: 0.3333 - val_loss: 1.0842 - val_accuracy: 0.3333\n",
            "Epoch 7/50\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0307 - accuracy: 0.3500 - val_loss: 1.0254 - val_accuracy: 0.3667\n",
            "Epoch 8/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9780 - accuracy: 0.3917 - val_loss: 0.9669 - val_accuracy: 0.4333\n",
            "Epoch 9/50\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.9325 - accuracy: 0.4583 - val_loss: 0.9290 - val_accuracy: 0.5000\n",
            "Epoch 10/50\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9009 - accuracy: 0.5667 - val_loss: 0.8975 - val_accuracy: 0.6000\n",
            "Epoch 11/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8748 - accuracy: 0.6833 - val_loss: 0.8684 - val_accuracy: 0.8333\n",
            "Epoch 12/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8501 - accuracy: 0.7750 - val_loss: 0.8456 - val_accuracy: 0.8333\n",
            "Epoch 13/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8305 - accuracy: 0.8000 - val_loss: 0.8272 - val_accuracy: 0.8333\n",
            "Epoch 14/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8115 - accuracy: 0.8000 - val_loss: 0.8061 - val_accuracy: 0.8000\n",
            "Epoch 15/50\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7944 - accuracy: 0.8250 - val_loss: 0.7892 - val_accuracy: 0.8000\n",
            "Epoch 16/50\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7777 - accuracy: 0.8167 - val_loss: 0.7738 - val_accuracy: 0.8000\n",
            "Epoch 17/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7635 - accuracy: 0.8167 - val_loss: 0.7607 - val_accuracy: 0.8333\n",
            "Epoch 18/50\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7491 - accuracy: 0.8167 - val_loss: 0.7427 - val_accuracy: 0.8333\n",
            "Epoch 19/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7346 - accuracy: 0.8250 - val_loss: 0.7278 - val_accuracy: 0.8333\n",
            "Epoch 20/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7207 - accuracy: 0.8167 - val_loss: 0.7143 - val_accuracy: 0.8333\n",
            "Epoch 21/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7078 - accuracy: 0.8250 - val_loss: 0.7024 - val_accuracy: 0.8333\n",
            "Epoch 22/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6951 - accuracy: 0.8333 - val_loss: 0.6895 - val_accuracy: 0.8333\n",
            "Epoch 23/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6842 - accuracy: 0.8167 - val_loss: 0.6752 - val_accuracy: 0.8333\n",
            "Epoch 24/50\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6714 - accuracy: 0.8083 - val_loss: 0.6650 - val_accuracy: 0.8333\n",
            "Epoch 25/50\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6655 - accuracy: 0.8333 - val_loss: 0.6579 - val_accuracy: 0.8333\n",
            "Epoch 26/50\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6500 - accuracy: 0.8417 - val_loss: 0.6448 - val_accuracy: 0.8333\n",
            "Epoch 27/50\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6403 - accuracy: 0.8333 - val_loss: 0.6293 - val_accuracy: 0.8000\n",
            "Epoch 28/50\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6301 - accuracy: 0.8083 - val_loss: 0.6203 - val_accuracy: 0.8333\n",
            "Epoch 29/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6213 - accuracy: 0.7917 - val_loss: 0.6107 - val_accuracy: 0.8333\n",
            "Epoch 30/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6111 - accuracy: 0.8000 - val_loss: 0.6009 - val_accuracy: 0.8333\n",
            "Epoch 31/50\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6028 - accuracy: 0.8250 - val_loss: 0.5939 - val_accuracy: 0.8333\n",
            "Epoch 32/50\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.5940 - accuracy: 0.8333 - val_loss: 0.5831 - val_accuracy: 0.8333\n",
            "Epoch 33/50\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.5866 - accuracy: 0.7917 - val_loss: 0.5735 - val_accuracy: 0.8333\n",
            "Epoch 34/50\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.5786 - accuracy: 0.8417 - val_loss: 0.5699 - val_accuracy: 0.8333\n",
            "Epoch 35/50\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.5703 - accuracy: 0.8500 - val_loss: 0.5605 - val_accuracy: 0.8333\n",
            "Epoch 36/50\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.5628 - accuracy: 0.8417 - val_loss: 0.5523 - val_accuracy: 0.8333\n",
            "Epoch 37/50\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.5576 - accuracy: 0.8250 - val_loss: 0.5451 - val_accuracy: 0.8333\n",
            "Epoch 38/50\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.5494 - accuracy: 0.8333 - val_loss: 0.5376 - val_accuracy: 0.8333\n",
            "Epoch 39/50\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.5428 - accuracy: 0.8333 - val_loss: 0.5310 - val_accuracy: 0.8333\n",
            "Epoch 40/50\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5413 - accuracy: 0.8000 - val_loss: 0.5218 - val_accuracy: 0.8000\n",
            "Epoch 41/50\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.5308 - accuracy: 0.8417 - val_loss: 0.5194 - val_accuracy: 0.8333\n",
            "Epoch 42/50\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5252 - accuracy: 0.8417 - val_loss: 0.5139 - val_accuracy: 0.8333\n",
            "Epoch 43/50\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.5211 - accuracy: 0.8583 - val_loss: 0.5110 - val_accuracy: 0.9000\n",
            "Epoch 44/50\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.5151 - accuracy: 0.8667 - val_loss: 0.5038 - val_accuracy: 0.8333\n",
            "Epoch 45/50\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.5101 - accuracy: 0.8500 - val_loss: 0.4944 - val_accuracy: 0.8667\n",
            "Epoch 46/50\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.5045 - accuracy: 0.8417 - val_loss: 0.4892 - val_accuracy: 0.8667\n",
            "Epoch 47/50\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.5013 - accuracy: 0.8417 - val_loss: 0.4868 - val_accuracy: 0.8333\n",
            "Epoch 48/50\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4954 - accuracy: 0.8417 - val_loss: 0.4798 - val_accuracy: 0.8667\n",
            "Epoch 49/50\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4906 - accuracy: 0.8500 - val_loss: 0.4747 - val_accuracy: 0.8667\n",
            "Epoch 50/50\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4872 - accuracy: 0.8583 - val_loss: 0.4726 - val_accuracy: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q Why is epochs 50 and batch_size 10?**\n",
        "\n",
        "These are hyperparameters. epochs represents how many times the dataset will be passed forward and backward through the network.\n",
        "\n",
        "batch_size is the number of training examples used in one iteration. The chosen values are just starting points and might not be optimal; in practice, these values should be tuned for best performance."
      ],
      "metadata": {
        "id": "vrdVWvvsReu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Evaluate the model:**"
      ],
      "metadata": {
        "id": "vR749ILeM7Gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluates the model's performance on the test data.\n",
        "# loss: The loss value of the model on the test data.\n",
        "# accuracy: The accuracy of the model on the test data.\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dfUgKUFLjyr",
        "outputId": "ccb0881c-ca59-47a9-ac87-2205a6b59d4a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 239ms/step - loss: 0.4726 - accuracy: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prints the loss and accuracy of the model on the test dataset.\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ptN3gWiNIzT",
        "outputId": "7b964d2e-a7e4-45ab-9cc9-3086f86e93e9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.47258517146110535\n",
            "Test Accuracy: 0.8333333134651184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q Why consider only accuracy in metrics?**\n",
        "\n",
        "Accuracy is a straightforward metric for classification problems. It gives a general idea of how well the model is performing. Depending on the problem, other metrics (like precision, recall, F1-score) might also be relevant. In this simple example, accuracy suffices to demonstrate model performance."
      ],
      "metadata": {
        "id": "qSSa-H-qRvBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. How is loss and accuracy calculated?**\n",
        "\n",
        "Loss: Represents how far the predictions of the model are from the true values. The categorical_crossentropy loss is commonly used for multi-class classification. It quantifies the difference between the predicted probability and the true class.\n",
        "\n",
        "\n",
        "Accuracy: It's a metric that calculates the proportion of correctly predicted classification outcomes in the dataset. For each prediction, if the maximum index in the predicted vector matches the maximum index in the true vector, then it's considered a correct prediction."
      ],
      "metadata": {
        "id": "kmdi1BHSPoHc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IOqmauw8NReM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}